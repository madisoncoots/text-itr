---
title: "Coots/Deschler Initial Analysis"
output: pdf_document
header-include: 
  - \usepackage{setspace}\doublespacing
  - \newcommand\E{\mathbb{E}}
  - \newcommand\V{\mathbb{V}}
  - \newcommand\cN{\mathcal{N}}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries and setup
library(evalITR)
library(dplyr)
library(haven)

# read data
df <- read_dta("~/Desktop/HKS/G1 Fall/Gov 2003/Final Paper/Replication package/Field experiment data and code/TextMessageData.dta")

# subset to only those with the newform?
df <- df[df$NewForm == 1,]

# Create Treatment column (original does not work for some reason)
df$Treatment <- df$consequences + df$planmaking + df$combination

# Flip FTA because a success is actually appearing
df$success <- 1 - df$FTA


control <- df[df$Treatment == 0,]
treated_overall <- df[df$Treatment == 1,]
cons <- df[df$consequences == 1,]
plan <- df[df$planmaking == 1,]
combo <- df[df$combination == 1,]
```

Initial results table:
\begin{tabular}{l | c c c c | r}
  & No text & Planmaking & Consequences & Combination & Overall \\ \hline
  Appeared & `r format(nrow(control) - sum(control$FTA), scientific = F)` & `r nrow(plan) - sum(plan$FTA)` & `r nrow(cons) - sum(cons$FTA)` & `r nrow(combo) - sum(combo$FTA)` & `r format(nrow(df) - sum(df$FTA), scientific = F)` \\
  Failed to Appear & `r format(sum(control$FTA), scientific = F)` & `r sum(plan$FTA)` & `r sum(cons$FTA)` & `r sum(combo$FTA)` & `r format(sum(df$FTA), scientific = F)` \\ \hline
  Appearance rate & `r round(1 - (sum(control$FTA)/nrow(control)), 3)` & `r round(1 - (sum(plan$FTA)/nrow(plan)), 3)` & `r round(1 - (sum(cons$FTA)/nrow(cons)), 3)` & `r round(1 - (sum(combo$FTA)/nrow(combo)), 3)` & `r round(1 - (sum(df$FTA)/nrow(df)), 3)`
\end{tabular}

Treating each treatment independently but with the same large control group, let $\pi_{jk}$ be the proportion of individuals in the principal stratum where $(Y_i(0), Y_i(1)) = (j,k)$. Then, for any set of constraints on the strata, we know that the possible range of the graduation rate, $G$ is $[\text{min}(\pi_{11}), \text{max}(1-\pi_{00})]$. Additionally, we can write the possible range if we make the monotonicity assumption
Using the repro, we can then write the range of  possible appearance rates under each treatment type. Additionally, we can write the possible range if we make the monotonicity assumption $Y_i(1) \geq Y_i(0)$. For various reasons with this data, we believe the monotonicity assumption is not a good one to make. There are a multitude of theoretical mechanisms by which a text message could cause individuals to _not_ appear in court, thus resulting in $Y_i(0) > Y_i(1)$. These could include, but are not limited to, messages scaring individuals due to the consequential nature, TODO.

\begin{tabular}{r | l l}
Treatment & No assumption & Monotonicity assumption \\ \hline
Planmaking & [.275, 1] & [.593,.682] \\
Consequences & [.303, 1] & [.593, .710] \\
Combination & [.314, 1] & [.593, .721]
\end{tabular}

### Individualized Treatment Rules
The main focus of our analysis is to identify various individualized treatment rules ("ITRs"), and compare them to the experimental results using techniques from Imai and Li (2021).

* Simple ITR - by borough (what is the best we can do?)
* Simple ITR - by racial composition (what is the best we can do?)
* ITR from Imai and Strauss (2011)?
* ML ITR

### Exploratory stuff
```{r include = F}
# Function to easily extract appearance rates
appearance_rate_by_demo <- function(data, treatment = "All", demo = "All", threshold = 0.5, places = 3) {
  if (demo == "All" & treatment == 'All')
    tmp <- data
  else if (demo == "All")
    tmp <- data[which(data[treatment] == 1),]
  else if  (treatment == "All")
    tmp <- data[which(data[demo] > threshold),]
  else
    tmp <- data[which(data[demo] > threshold & data[treatment] == 1),]
  return(round(1 - (sum(tmp$FTA) / nrow(tmp)), places))
}
```

### Subgroup analysis
The original dataset did not contain a variable for the race of an individual, but did contain the percentage of the population in each individual's ZIP code that was either black or Hispanic. We can construct a approximation of an individual's race by coding someone as, for example, black if more than 50% of the population in that ZIP code was black. We can do the same based on the percentage below the poverty line column. One could argue that we should actually be using a lower threshold than 50% given that communities of color and poor communities tend to be overpoliced, but without further data, we stick to the 50% threshold. The table below summarizes the appearance rates for various demographics in the data, using the heuristic just described.

```{r include=F}
# Demographic Breakdowns
df2 <- df
df2['Male'] = 1 - df2$Female
df2['PctBlackHispZip'] = df2$PctBlackZip + df2$PctHispZip
demos = c('Male', "Female", "PctBlackZip", "PctHispZip", "PctBlackHispZip", "PctBelowPovZip")
res <- matrix(data = 0, nrow = length(demos), ncol = 4)
for (d in  1:length(demos)) {
  res[d, 1] <- appearance_rate_by_demo(df2, demo = demos[d])
  res[d, 2] <- appearance_rate_by_demo(df2, treatment = "planmaking", demo = demos[d])
  res[d, 3] <- appearance_rate_by_demo(df2, treatment = "consequences", demo = demos[d])
  res[d, 4] <- appearance_rate_by_demo(df2, treatment = "combination", demo = demos[d])
}
```
\begin{tabular}{l r | c | c c c}
Demo. & Group & Overall & Planmaking & Consequences & Combination \\ \hline
Gender & Male & `r res[1,1]` & `r res[1,2]` & `r res[1,3]` & `r res[1,4]` \\
 & Female & `r res[2,1]` & `r res[2,2]` & `r res[2,3]` & `r res[2,4]` \\ \hline
Race & Black & `r res[3,1]` & `r res[3,2]` & `r res[3,3]` & `r res[3,4]` \\
 & Hispanic & `r res[4,1]` & `r res[4,2]` & `r res[5,3]` & `r res[4,4]` \\
 & Black or Hispanic & `r res[5,1]` & `r res[5,2]` & `r res[5,3]` & `r res[5,4]` \\ \hline
Poverty & Below Poverty Line & `r res[6,1]` & `r res[6,2]` & `r res[6,3]` & `r res[5,4]`
\end{tabular}

\smallskip

We can likewise break down the appearance rate by NYC borough:

```{r include=F}
# Demographic Breakdowns
df2 <- df
demos = c('Bronx', "Brooklyn", "Manhattan", "Queens", "StatenIsland")
res <- matrix(data = 0, nrow = length(demos), ncol = 5)
for (d in  1:length(demos)) {
  res[d, 1] <- appearance_rate_by_demo(df2, demo = demos[d])
  res[d, 2] <- appearance_rate_by_demo(df2, treatment = "Treatment", demo = demos[d])
  res[d, 3] <- appearance_rate_by_demo(df2, treatment = "planmaking", demo = demos[d])
  res[d, 4] <- appearance_rate_by_demo(df2, treatment = "consequences", demo = demos[d])
  res[d, 5] <- appearance_rate_by_demo(df2, treatment = "combination", demo = demos[d])
}
```
\begin{tabular}{r | c | c | c c c}
Borough & Overall & Any Treatment & Planmaking & Consequences & Combination \\ \hline
The Bronx & `r res[1,1]` & `r res[1,2]` & `r res[1,3]` & `r res[1,4]` & `r res[1,5]` \\
Brooklyn & `r res[2,1]` & `r res[2,2]` & `r res[2,3]` & `r res[2,4]` & `r res[2,5]` \\ 
Manhattan & `r res[3,1]` & `r res[3,2]` & `r res[3,3]` & `r res[3,4]` & `r res[3,5]`  \\
Queens & `r res[4,1]` & `r res[4,2]` & `r res[4,3]` & `r res[4,4]` & `r res[4,5]`  \\
Staten Island & `r res[5,1]` & `r res[5,2]` & `r res[5,3]` & `r res[5,4]` & `r res[5,5]`
\end{tabular}

TODO interpret

## For First Write-up

```{r include = FALSE}
# test DF with simple ITR:
# - SI gets planmaking
# - Bronx gets consequences
# - Brooklyn gets combination
# - Manhattan and Queens are control

df_boroughs <- df[!is.na(df$StatenIsland),]
df_boroughs$itr_plan <- df_boroughs$StatenIsland
df_boroughs$itr_cons <- df_boroughs$Bronx
df_boroughs$itr_combo <- df_boroughs$Brooklyn

pape_matrix <- function(d, itr_cols = c('itr_plan', 'itr_cons', 'itr_combo')) {
  itr_treat <- d$itr_plan + d$itr_cons + d$itr_combo
  appeared <- 1 - d$FTA

  names <- list("Any Text", "Planmaking", "Consequences", "Combination")
  orig <- data.frame(d$Treatment, d$planmaking, d$consequences, d$combination)
  itr <- data.frame(itr_treat, d[itr_cols[1]], d[itr_cols[2]], d[itr_cols[3]])
  pape_mat <- matrix(nrow = 4, ncol = 4, c(names, names))
  sd_mat <- matrix(nrow = 4, ncol = 4, c(names, names))
  for (r in 1:length(names)) {
    for (c in 1:length(names)) {
      if (r == c) {
        pape_mat[r,c] = 0
        sd_mat[r,c] = 0
      }
      else {
        p = PAPE(orig[,r], itr[,c], appeared, centered = FALSE)
        pape_mat[r,c] <- p$pape
        sd_mat[r,c] <- p$sd
      }
    }
  }
  rownames(pape_mat) <- names
  colnames(pape_mat) <- names
  rownames(sd_mat) <- names
  colnames(sd_mat) <- names
  return(tibble(pape = pape_mat, sd = sd_mat))
}
p <- pape_matrix(df_boroughs)
```

## First Causal Forest Attempt - Binary
```{r eval=FALSE}
library(grf)
# covariate set, leave one borough out
# offense-based covariates
X_offense <- c("StatenIsland", "Bronx", "Brooklyn", "Queens", "Park", "Alcohol", "Marijuana",
               "DisorderlyActive", "DisorderlyPassive", "Disorderly", "Bike", "Noise",
               "MotorVehicle", "Urination")
# individual-vased covariates
X_perp <- c("Female", "Age", "PastFTA", "NbPastFTA", "NbPastSummons",  "PastSummons", 
            "MedianIncZip", "PctBlackZip", "PctHispZip", "PctBelowPovZip", "DOW")
# all covariates
X_full <- c(X_offense, X_perp)

# fit forests
trees <- 1000
forest_bin_full <- causal_forest(df[X_full], as.vector(df$success), 
                                 as.vector(df$Treatment), num.trees = trees)
forest_bin_offense <- causal_forest(df[X_offense], as.vector(df$success),
                                    as.vector(df$Treatment), num.trees = trees)
forest_bin_perp <- causal_forest(df[X_perp], as.vector(df$success), 
                                 as.vector(df$Treatment), num.trees = trees)


# create dataframe of predictions
forests <- list(forest_bin_full, forest_bin_offense, forest_bin_perp)
names <- c("All", "Offense", "Perpetrator")
res_df <- data.frame(matrix(NA, nrow = nrow(df), ncol = 0))
for (n in 1:length(names)) {
  f <- forests[[n]]
  pred <- predict(f, estimate.variance = TRUE)
  taus <- pred$predictions
  vars <- pred$variance.estimates
  res_df[paste(names[n], "_Est", sep = '')] <- taus
  res_df[paste(names[n], "_Var", sep = '')] <- vars
}
# write csv of all forests
write.csv(res_df, paste("~/Desktop/HKS/G1 Fall/Gov 2003/Final Paper/text-itr/binary_", trees,
                        "trees_results.csv", sep = ''), row.names = FALSE)
```

### Calculate actual ITRs from the forests
```{r eval=FALSE}
trees = c(100, 500, 1000)
res <- read.csv(paste("~/Desktop/HKS/G1 Fall/Gov 2003/Final Paper/text-itr/binary_", trees,
                        "trees_results.csv", sep = ''))
names <- c("All", "Offense", "Perpetrator")
itrs <- data.frame(matrix(NA, nrow = nrow(res), ncol = 0))

for (t in trees) {
  for (n in names) {
    taus <- res[paste(n, "_Est", sep = '')]
    vars <- res[paste(n, "_Var", sep = '')]
    itrs[paste(n, "_simple_", t, sep = '')] <- as.numeric(taus > 0)
    itrs[paste(n, "_harmless_", t, sep = '')] <- as.numeric(taus - (1.96 * sqrt(vars)) > 0)
  }
}

write.csv(itrs, "~/Desktop/HKS/G1 Fall/Gov 2003/Final Paper/text-itr/binary_itrs.csv", row.names = FALSE)
```

### Calculate PAPE for binary ITRs
```{r}
itrs <- read.csv("~/Desktop/HKS/G1 Fall/Gov 2003/Final Paper/text-itr/binary_itrs.csv")
papes <- c()
sds <- c()
covariates <- c()
rule <- c()
numtrees <- c()
for (r in colnames(itrs)) {
  papelist <- PAPE(df$Treatment, itrs[,r], df$success, centered = FALSE)
  covariates <- c(covariates, strsplit(r, '_')[[1]][1])
  rule <- c(rule, strsplit(r, '_')[[1]][2])
  numtrees <- c(numtrees, strsplit(r, '_')[[1]][3])
  papes <- c(papes, papelist$pape)
  sds <- c(sds, papelist$sd)
}
pape_df = data.frame(covariates, numtrees, rule, papes, sds)
write.csv(pape_df, "~/Desktop/HKS/G1 Fall/Gov 2003/Final Paper/text-itr/binaryITR_pape.csv", row.names = F)
```
## Multi-arm Causal Forest Attempt
```{r eval=FALSE}
trees <- c(100, 500, 1000)
# create single treatment array, 1 is planmaking, 2 is consequences, 3 is combo
treat_type <- df$planmaking + 2*df$consequences + 3*df$combination

multi_arm <- multi_arm_causal_forest(df[X_full], as.vector(df$success), 
                                     factor(treat_type), num.trees = 100)
preds <- predict(multi_arm, estimate.variance = T)
taus <- preds$predictions[,,]
vars <- preds$variance.estimates
treat_mat <- matrix(0, nrow = nrow(vars), ncol = ncol(vars))
for (r in 1:nrow(vars)) {
  for (c in 1:ncol(vars)) {
    treat_mat[r,c] <- as.numeric(taus[r,c] - 1.96 * sqrt(vars[r,c]) > 0)
  }
}
treat_df <- data.frame(treat_mat)
colnames(treat_df) <- c("planmaking", "consequences", "combination")
```


